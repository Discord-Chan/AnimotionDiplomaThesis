\section{Three.js}
\setauthor{Romeo Bhuiyan}
\textbf{Three.js} is a JavaScript library that allows for the creation of interactive 3D graphics in the web 
browser. It is designed to work seamlessly with WebGL, which is a web-based 3D graphics API that allows 
developers to access the hardware-accelerated graphics capabilities of a user's computer. Three.js 
makes it possible to create rich 3D experiences with JavaScript and HTML, without the need for any plugins or downloads.
With Three.js, developers can create a wide variety of 3D scenes and animations, including but not limited to: 
realistic environments with dynamic lighting and shadows, complex 3D models and animations, interactive 3D 
games and simulations, and virtual reality (VR) and augmented reality (AR) experiences.
\\
\\
The library provides a large number of powerful features and abstractions that simplify 
the process of creating 3D graphics. It includes a number of built-in shapes, such as cubes, 
spheres, and cylinders, as well as more complex shapes that can be constructed from the 
vertices and faces of custom 3D models. Three.js also includes a number of materials that 
allow for realistic shading and texturing of 3D objects, as well as lights and cameras 
that can be positioned and manipulated to achieve the desired lighting and perspective effects.
Another important aspect of Three.js is its support for keyframe animations. This allows
 developers to animate objects over time, creating smooth transitions and movements. 
 The library also provides support for physics simulations, making it possible to 
 create complex and interactive 3D experiences that respond to user input.
\\
\\
\subsection{Three-vrm.js}
\setauthor{Romeo Bhuiyan}
\textbf{Three-vrm.js} is a JavaScript library that provides support for rendering 
and manipulating VRM (Virtual Reality Model) characters in web-based environments. 
It is built on top of Three.js, a widely-used library for creating 3D graphics and 
animations in web browsers.
Three-vrm.js offers a set of tools and APIs for working with VRM models, allowing 
developers to easily import, manipulate and animate VRM characters in their web-based 
projects. This includes support for importing VRM files, as well as manipulating and animating 
individual bones, rigging, morph targets, and materials of the characters as shown below in the figure \ref{fig:threevrmdebug}.
In addition to basic VRM functionality, three-vrm.js also includes a set of VRM-specific 
features and extensions. For example, it provides support for VRM's humanoids, which define 
the structure of a VRM character and allow for easy animation of the character. It also includes 
support for VRM's blend shapes, which are used to manipulate the shape of the character's 
face and expressions.
\\
\\
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/threevrmdebug.PNG}
    \caption{Three-vrm.js debug mode}
    \label{fig:threevrmdebug}
\end{figure}
\\
\\
\section{Decision between Webframeworks}
\setauthor{Christoph Lasinger}

\subsection{Angular}
\setauthor{Christoph Lasinger}
SPA stands for Single-Page application and is a type of web application that dynamically rewrites the 
current webpage instead of entirely reloading, i.e., requesting and displaying data from the server, it. 
The main benefits of an SPA are a more responsive user experience and avoiding the delay of having to wait 
for a website to load. Angular, the next version of the discontinued AngularJS, is a framework for such 
applications and is maintained and released by Google  in 2016.
\\
\\
Angular is a free and open-source project, meaning that the copyright owner, in this case the company Google,
gives anyone the right to distribute, modify and study the software for any purpose. Open-source software is 
however not the same as public-domain software as it still possesses a copyright, just a very open one, hence its name. 
One big advantage of such software over closed-source software is that anyone in the world can find bugs and make 
improvements for it, free of charge for the actual project-owner. Moreover, even if the publisher decides to no longer 
provide support and update their software, it can still be adapted and improved by its users. Amazingly, it can be seen 
in practise that this concept works and users really do improve open-source projects not only for their benefit but for 
that of others as well. For those reasons, Animotion too is an open-source project.\cite{Opensource}
%write more concretely about Angular
\\
\\
\subsection{React}
\setauthor{Christoph Lasinger}
Like Angular, React also is an open-source SPA-Framework, maintained and released by Meta (then Facebook) in 2013.
The easiest way to set up a React project is to use the officially supported command "create-react-app" 
(similar command for Next.js: create next-app) as it offers a modern build setup and is already preconfigured. 
Optionally, further parameters can be used to customize the project, for example "—template typescript" to use TypeScript 
instead of JavaScript. After the project is created, it can immediately be started and viewed on localhost using either 
"npm start" or "yarn start", depending on the package manager that is used. For development the command "npm run dev" is 
also very useful, as the website is immediately refreshed after saving any changes to the code. \cite{create-react}
% (is run dev also available to react or just nextjs?)
\\
\\
\subsection{Next.js}
\setauthor{Christoph Lasinger}
Hier werden noch Sachen eingefügt!
\\
\\
\subsection{Sass}
\setauthor{Christoph Lasinger}
Sass stands for syntactically awesome style sheets and is a stylesheet language that compiles into CSS (Cascading Style Sheets). 
It extends CSS by quite a few features such as functions, variables, nested loops, mixins, etc. As a result, Sass enables 
stylesheets to be organized more easily and for design to be shared within and across projects. 
For example, instead of copying the hexadecimal value representing a colour for each and every element and possibly making mistakes, 
Sass makes it possible to store the colour code in a variable and access it using its more human-readable name.
Another useful feature of Sass not mentioned before is inheritance, an example of which can be seen in the following code snippet.
% insert sass code snippet here
Through the "extend" keyword CSS properties of one selector can be passed over to another selector which then automatically has those 
same properties set to the same values, thus inheriting them. In this example, legal-notice-text is merely a variant of the 
default-text and should therefore be in the same colour, font, and alignment. However, as legal-notice-text is of not much 
importance to the average user of the website, it should be rather small and take up little space. Because of the usage of 
inheritance, the code becomes more readable and easier to write and understand. \cite{SassInheritance}
\\
\\
\section{Comparison of 3D rendering technologies}
\setauthor{Romeo Bhuiyan}
In order to display a virtual model on a canvas, a 3D rendering technology was required. Two libraries, 
GLTFLoader and WebGL, were evaluated using JavaScript to determine the best fit for the task at hand. 
The choice between the two ultimately depends on the specific needs and limitations of the project. 
Both GLTFLoader and WebGL are effective tools for rendering, but each have their own 
unique strengths and weaknesses.
\\
\\
\subsection{WebGL}
\setauthor{Romeo Bhuiyan}
\textbf{WebGL} is designed to work seamlessly with other web technologies such as HTML, CSS, and JavaScript, 
making it easy to integrate 3D graphics into web pages. It allows developers to create a wide range 
of interactive 3D applications and visualizations, including games, scientific 
simulations, data visualizations, and more.
One of the key features of WebGL is its ability to take full advantage of the GPU 
(graphics processing unit) on the user's device. This allows WebGL applications to 
run smoothly and efficiently, even on devices with limited resources. Additionally, WebGL provides 
a high level of compatibility across different browsers and devices, making it a widely accessible technology.
WebGL is supported by most modern web browsers, including Chrome, Firefox, Safari, and Edge. 
This means that developers can create WebGL applications that can be easily accessed by 
users on a wide range of devices and platforms.
\\
\\
\subsection{GLTFLoader}
\setauthor{Romeo Bhuiyan}
\textbf{GLTFLoader} is a JavaScript library that allows developers to load and parse 3D models in the GLTF 
(GL Transmission Format) file format. GLTF is a widely adopted file format for 3D models and is 
supported by many 3D modeling software, including Blender and SketchUp.
GLTFLoader provides a simple and easy-to-use API for loading and parsing GLTF models, 
which can then be displayed using WebGL. The library takes care of all the complex tasks 
involved in loading and parsing the model data, such as handling binary buffers, parsing 
JSON data and creating the geometry and materials for the model.
GLTFLoader also provides support for advanced features such as animations, skinning and morph 
targets, which are typically found in more complex 3D models. This allows developers to create 
more advanced and interactive 3D applications and visualizations.
One of the key benefits of using GLTFLoader is that it greatly simplifies the process of loading and 
displaying 3D models on the web. By using a standardized file format and a dedicated library, 
developers can focus on creating the logic and functionality of their application, rather than 
spending time on complex parsing and loading tasks.
Additionally, GLTF is a highly efficient format, which means that it can be loaded faster, 
and also the files are smaller in size, making it a great choice for loading 3D models on the web.
\\
\\
\subsection{Conclusion}
In conclusion, GLTFLoader is a powerful tool for creating 3D graphics for the web that 
offers several advantages over WebGL. One of the main benefits of using GLTFLoader is 
its ability to handle the loading and parsing of GLTF (GL Transmission Format) files, 
which are a widely-used format for 3D models. This eliminates the need for developers 
to write their own code to handle file loading and parsing, saving time and effort. 
Additionally, GLTFLoader provides a more streamlined and convenient way to work with 
3D models, as it allows developers to easily import and use pre-existing models rather 
than having to create them from scratch. Furthermore, GLTFLoader also supports various 
features such as animations, cameras, lights and many more which can be easily integrated 
with web pages. Therefore, it is more efficient and convenient for the purpose of Animotion 
to work with 3D models on the web, GLTFLoader is the best choice as seen below in the figure. \ref{fig:gltfloader}
\\
\\
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/GLTFLoader.png}
    \caption{Loading the model with the help of GLTFLoader}
    \label{fig:gltfloader}
\end{figure}
\newpage
\section{Object interaction with OrbitControls}
\author{Romeo Bhuiyan}
\texttt{Orbit-Controls.js} is a JavaScript library used for controlling camera rotations 
and movements in 3D graphics applications. This library provides an easy-to-use 
interface for controlling the camera in a 3D environment, and is commonly used in 
interactive 3D web applications and other 3D visualizations.

The library works by allowing the user to control the camera through mouse 
and touch inputs, such as panning, zooming, and rotating the camera around a 
certain point in the 3D space. It uses the Three.js library, which is a popular 
library for creating 3D graphics in the browser, as a base.

Orbit-Controls.js makes it easy to interact with 3D scenes and view them from different 
perspectives. It provides a simple way to navigate and explore 3D environments, making it a 
useful tool for a wide range of applications, including scientific simulations, product visualization, architecture, and more.

To use \texttt{Orbit-Controls.js}, you need to include the library in your HTML file, and then 
create an instance of the OrbitControls class. You will also need to pass in a Three.js 
camera object, as well as a Three.js renderer. The library will then take care of the rest, 
allowing you to control the camera using mouse and touch inputs.
\\
\\
\subsection{Azimuth- and polar angle}
\setauthor{Romeo Bhuiyan}
The Azimuth angle and the Polar angle are two important concepts in 3D graphics and visualization. 
In the context of orbit-controls.js, they are used to control the camera's rotation and position in 3D space.

The Azimuth angle refers to the angle between a reference plane and the position of the camera in 
the x-z plane. This angle determines the camera's horizontal rotation and the direction it is facing. 
In orbit-controls.js, the Azimuth angle is controlled by moving the mouse horizontally.

The Polar angle, on the other hand, refers to the angle between the position of the camera and the 
positive y-axis. It determines the camera's vertical rotation and the height of the camera relative 
to the scene. In orbit-controls.js, the Polar angle is controlled by moving the mouse vertically.

By controlling both the Azimuth and Polar angles, the orbit-controls.js library allows the user to 
change the position and orientation of the camera in 3D space. This is especially useful for interactive 
3D visualizations, where the user needs to be able to explore the scene from different perspectives.

Orbit-controls.js is often used in conjunction with WebGL and Three.js, which are libraries for creating 
3D graphics and animations in the browser. The combination of these technologies makes it possible to create 
interactive 3D visualizations that can run in a web browser, without the need for additional software or plugins.
\\
\begin{figure}[htb]
  \centering
  \includegraphics[width=0.5\textwidth]{pics/orbitcontrols.png}
  \caption{Azimuth and Polar angle}
  \label{fig:orbitcontrols}
\end{figure}
\newpage
\section{Screen capture technology}
\setauthor{Romeo Bhuiyan}
Screen recording is an important aspect in the world of VRM programs. 
Screen recording allows for the capture of all the visual and audio elements that are displayed 
on a computer screen. The recorded content can then be shared, edited, and distributed for various 
purposes. In the context of Animotion, screen recording can serve several important functions.

First and foremost, screen recording can be used to document and showcase the functionality and 
capabilities of the VRM Model program. This can include recording the different animations, movements, 
and interactions that are possible within the program. This type of recording can be used to demonstrate 
the program's features to potential users, stakeholders, or clients, and can help to generate interest 
and engagement with the VRM Model program.

Screen recording can also be used to provide support and training to users of this kind of program. 
By recording step-by-step instructions or tutorials, users can learn how to use the program effectively, 
even if they have no prior experience or knowledge. Additionally, if users encounter any issues or 
difficulties while using the VRM Model program, a screen recording can be created to assist in resolving 
the problem and provide a visual representation of the issue at hand.

Screen recording can also be used to document bugs or other technical issues that arise during the use 
of the VRM Model program. By capturing the screen and audio output, developers and technical support 
teams can more easily diagnose and resolve any problems that may occur.
Lastly, screen recording can be used for creative purposes, such as creating animations or special 
effects using the VRM Model program. By capturing and manipulating the recordings, users can explore 
new ways to use and interact with the VRM Model program, and create new and unique content that 
can be shared and enjoyed by others.
\\
\\
\subsection{RecordRTC}
\setauthor{Romeo Bhuiyan}
\texttt{RTCRecord.js} is a JavaScript library that allows users to perform real-time communication (RTC) 
recording in the web browser. It uses the WebRTC API, which is a standardized way of enabling 
real-time communication capabilities in web browsers, to capture audio and video data from a 
user's microphone and webcam. The captured data can then be saved as a video file for future use.

One of the main use cases for \texttt{RTCRecord.js} is for screen recording in web-based applications, 
such as video conferencing and online collaboration tools. This allows users to record their 
screen activity, audio, and webcam footage in real-time, without the need for any additional 
software. The recorded videos can then be used for training purposes, for documentation, 
or for sharing information with others.

Another use case for \texttt{RTCRecord.js} is for creating educational content, such as tutorials 
and screencasts. With RTCRecord.js, users can easily record their screen and webcam footage, 
and share their knowledge with others in a visually engaging way. \cite{rtcrecord}
\\
\\
\subsection{FFmpeg}
\setauthor{Romeo Bhuiyan}
FFmpeg is an open-source software library for audio, video and image processing. 
The library can be used to encode, decode, transcode, mux, demux, stream, filter and play various 
multimedia files. It is written in the C programming language and is commonly used on a variety of 
operating systems including Windows, macOS, and Linux.

\texttt{FFmpeg.min.js} is a JavaScript implementation of FFmpeg, which can be used for client-side video 
processing in web applications. It provides a way for web developers to perform various multimedia 
operations in real-time within a web browser. With ffmpeg.min.js, developers can leverage the 
functionality of FFmpeg in their web applications, including video compression, resizing, cropping, and transcoding.

The library can be used for a range of purposes, including creating video editing tools, video 
compression utilities, and real-time video processing applications. With the rise of cloud computing, 
ffmpeg.min.js can be used to perform video processing operations in real-time on a server, without requiring 
the user to have FFmpeg installed on their local machine. This can save both time and resources, and provide 
a convenient solution for web developers looking to incorporate video processing capabilities into their applications. \cite{ffmpeg}
\\
\\
\subsection{WebM video format for web-based applications}
\setauthor{Romeo Bhuiyan}
The WebM video format is an open, royalty-free, and high-quality video compression format 
that has been specifically designed for the web. This format was developed by Google and 
the WebM Project, a community of developers and industry partners, with the goal of 
providing an alternative to proprietary video formats, such as H.264 and MPEG-4. 
The WebM format is based on the VP8 video codec and the Vorbis audio codec, 
which are both open-source and can be freely used without licensing fees.

One of the key benefits of the WebM format is its high-quality video compression, 
which provides visually lossless video playback without sacrificing file size. 
This makes WebM an ideal choice for multimedia applications that require 
high-quality video playback, such as video conferencing, video on demand, and live streaming. The format 
is also highly scalable and can be used on a wide range of devices, from desktop computers to mobile 
devices, ensuring compatibility across platforms.

Another important advantage of the WebM format is its compatibility with HTML5. The format is 
natively supported by most modern browsers, including Google Chrome, Mozilla Firefox, and Opera, 
and can be easily embedded into web pages without the need for additional plugins or software. 
This makes it an ideal choice for web-based video applications, as it eliminates the need for 
users to download and install additional software or plugins.

In addition to its compatibility and high-quality video compression, the WebM format also has 
a small file size, making it an ideal choice for applications that require fast loading times, 
such as online video platforms and websites. The format is also highly customizable, and developers 
can use the WebM SDK to modify and extend the format to meet their specific needs.
Another key benefit of the WebM format is its ability to support a wide range of video and audio 
codecs, including VP9, Opus, and VP10, making it a flexible choice for applications that require 
support for multiple codecs. The format also supports high dynamic range (HDR) video, providing 
users with a superior viewing experience with enhanced color and brightness. \cite{webm}
\\
\\
\section{Calculation of the virtual reality model} 
\setauthor{Romeo Bhuiyan}
Controlling a VRM on a website involves a complex process that involves 
capturing the user's inputs, updating the VRMs position and orientation in real-time, 
and rendering the updated model for display on the user's device.
The first step in this process is for the user to interact with the VR model 
using a device such as a VR headset or a mouse and keyboard. The inputs from the 
user are then captured by the website and sent to the server. The server then 
uses these inputs to update the VR model's position and orientation in real-time. 
\\
\\
Once the VR model has been updated, it is then rendered, and the new frame is 
sent back to the user's device for display. This process repeats as the user 
continues to interact with the VR model. 
To achieve this, various technologies are needed such as GLTFLoader\cite{GLTFLoader} from the THREE\cite{threejs}
libraries for rendering the VR model in the browser, WebSockets for real-time 
communication between the client and server, and a physics engine to simulate 
the movement and interactions of the VR model. 
\\
\\
\subsection{Blendshape}
\setauthor{Romeo Bhuiyan}
Blendshape is a technique used in 3D animation and computer graphics to create a smooth transition between different shapes or expressions of a 3D model. 
It works by creating a set of \emph{target} shapes for a 3D model, each representing a different expression or shape, 
and then using a set of weights or \emph{blend} values to interpolate between these target shapes, creating a 
smooth transition between them. This allows animators to create a wide range of expressive characters with a 
limited number of 3D models. Blendshapes are commonly used in animation, film and game industry.
\\
\\
\subsection{Lerp}
\setauthor{Romeo Bhuiyan}
The Lerp function, short for Linear Interpolation, is a mathematical function that is 
used to smoothly blend between two values. The function takes three arguments: a start value, 
an end value, and a weight. The weight is a value between 0 and 1 that represents the proportion 
of the blend between the start and end values.
For example, if the weight is set to 0, the function will return the start 
value, and if the weight is set to 1, the function will return the end value. 
If the weight is set to 0.5, the function will return the midpoint between the 
start and end values. By adjusting the weight, the Lerp function can be used to 
smoothly transition between any two values over time.
In the context of computer graphics, Lerp function is widely used for 
interpolating values such as position, rotation, and color, to make the 
animation smooth. It's also used in physics simulations, game development, 
and other fields where the smooth transition of values is needed.
\\
\\
\subsection{Determining mouth expressions}
\setauthor{Romeo Bhuiyan}

\begin{lstlisting}[language=Python,caption=Shape of mouth,label=lst:shapeOfMouth]
    for each shape in ["I", "A", "E", "O", "U"]
        Blendshape.setValue(PresetName[shape], 
        lerp(riggedFace.mouth.shape[shape], 
        Blendshape.getValue(PresetName[shape]), 0.5))

\end{lstlisting}
This pseudocode \ref{lst:shapeOfMouth} is a loop that iterates over an array containing the shapes \texttt{A, E, I, O, U}. 
The purpose of the loop is to blend the shape of a rigged face's mouth with a preset value of a 
Blendshape object for each of these shapes. The loop starts by setting the value of the Blendshape 
object to a blended value between the shape of the rigged face's mouth \texttt{(riggedFace.mouth.shape[shape])}
and the current value of the Blendshape object \texttt{(Blendshape.getValue(PresetName[shape]))}. 
The blend is done using the lerp function, which stands for linear interpolation. 
This function takes three arguments: the start value, the end value, and the weight of blending. 
In this case, the weight is set to 0.5, resulting in an even blend between the two values.
Once the blended value is calculated, it is set to the Blendshape object using the PresetName 
as the key. This process is repeated for each shape in the array, allowing the developer to 
easily blend the shape of the rigged face's mouth with preset values for multiple shapes.
\\
\\
\subsection{Blinking eyes}
\setauthor{Romeo Bhuiyan}
\begin{lstlisting}[language=Python,caption=Blinking of the eyes,label=lst:blinking]
    riggedFace.eye.l = lerp(clamp(1 - riggedFace.eye.l, 0, 1), Blendshape.getValue(PresetName.Blink), 0.5)
    riggedFace.eye.r = lerp(clamp(1 - riggedFace.eye.r, 0, 1), Blendshape.getValue(PresetName.Blink), 0.5)
    riggedFace.eye = Kalidokit.Face.stabilizeBlink(riggedFace.eye, riggedFace.head.y)
    Blendshape.setValue(PresetName.Blink, riggedFace.eye.l)

\end{lstlisting}
This pseudocode \ref{lst:blinking} is a code block that performs blinking animation on a VRM model. 
The code uses the Lerp function, the clamp function, and the stabilizeBlink function 
to animate the eyes of the rigged face.
\\
\\
The first two lines of the code use the Lerp function to blend the current value of the left and right eye shapes 
\texttt{(riggedFace.eye.l and riggedFace.eye.r)} with a preset Blink value from the Blendshape object. The Lerp 
function takes the clamp of the inversed eye shape values \texttt{(1 - riggedFace.eye.l and 1 - riggedFace.eye.r)}, 
which clamps the values to between 0 and 1, and the preset Blink value, which is obtained using \texttt{Blendshape.getValue(PresetName.Blink)}. 
The weight of the blend is set to 0.5, resulting in an even blend between the two values.
\\
\\
The third line of the code stabilizes the blinking animation of the eyes by calling 
the stabilizeBlink function from the \texttt{Kalidokit.Face} module. This function takes the 
current eye shape \texttt{(riggedFace.eye)} and the head's y position \texttt{(riggedFace.head.y)}  as arguments 
and returns a stabilized eye shape.
\\
\\
Finally, the fourth line of the code sets the Blink value of the Blendshape object to the 
stabilized left eye shape \texttt{(riggedFace.eye.l)}. This will result in a smooth and stable blinking  
of the eyes on the VRM model.
\\
\\
In summary, this pseudocode is a code block that animates the eyes of a VRM model by 
blending the current eye shape with a preset Blink value, stabilizing the blinking animation, 
and updating the Blink value of the Blendshape object. The code uses the Lerp function, the clamp 
function, and the stabilizeBlink function to create a smooth and stable blinking animation on the VRM model.
\\
\\
\subsection{Positioning the model}
\setauthor{Romeo Bhuiyan}

\begin{lstlisting}[language=Python,caption=Position of the model,label=lst:position]
    function rigPosition(name, position={x:0, y:0, z:0}, dampener=1, lerpAmount=0.3) {
        if (currentVrm is not defined) {
          return;
        }
        Part = currentVrm.humanoid.getBoneNode(name from THREE.VRMSchema.HumanoidBoneName);
        if (Part is not defined) {
          return;
        }
        vector = new THREE.Vector3(position.x * dampener, position.y * dampener, position.z * dampener);
        Part.position.lerp(vector, lerpAmount);
    }
      
      oldLookTarget = new THREE.Euler();
      function rigFace(riggedFace) {
        if (currentVrm is not defined) {
          return;
        }
        rigRotation("Neck", riggedFace.head, 0.7);
      
        Blendshape = currentVrm.blendShapeProxy;
        PresetName = THREE.VRMSchema.BlendShapePresetName;
    }
      

\end{lstlisting}
This pseudocode \ref{lst:position} is a code block that positions a VRM model according to a set of defined parameters. 
The code block consists of two functions: rigPosition and rigFace.
\\
\\
The \textbf{rigPosition function} takes three arguments: name, position, dampener, and lerpAmount. The name argument is the name 
of the part of the VRM model that needs to be positioned, the position argument is an object with x, y, and z properties 
that represent the desired position of the part, the dampener argument scales the position values, and the lerpAmount 
argument is the weight of the blend between the current position and the desired position.
The function starts by checking if the currentVrm is defined, and if it's not, the function returns without executing further code. 
If currentVrm is defined, the code uses the \texttt{humanoid.getBoneNode} method to retrieve the part specified by the name argument. 
If the part is not defined, the function returns without executing further code.
If the part is defined, the code creates a new \texttt{THREE.Vector3} object with the desired position 
values (scaled by the dampener argument), and the Part.position property is blended towards the 
desired position using the lerp method. The weight of the blend is set by the lerpAmount argument.
\\
\\
The \textbf{rigFace function} takes one argument: riggedFace, which represents the face of the VRM model. The function starts 
by checking if the \emph{currentVrm} is defined and if it's not, the function returns without executing further code.
If \emph{currentVrm} is defined, the function calls the rigRotation method and passes two arguments: \emph{Neck} and \emph{riggedFace.head}, 
with a third optional argument of 0.7. This rotates the neck of the VRM model according to the head position.
The function also defines the Blendshape variable and assigns it to the \texttt{blendShapeProxy} property of the \emph{currentVrm} object. 
The \texttt{PresetName} variable is set to the \texttt{BlendShapePresetName} property from the \texttt{THREE.VRMSchema} object.
\\
\\
\subsection{Animating the body}
\setauthor{Romeo Bhuiyan}
\begin{lstlisting}[language=Python,caption=Animating the rest of the body,label=lst:restbody]
    if (pose2DLandmarks && pose3DLandmarks) {
    riggedPose = solvePose(pose3DLandmarks, pose2DLandmarks, "mediapipe", videoElement);
    rotate("Hips", riggedPose.Hips.rotation, 0.7);
    position("Hips", -riggedPose.Hips.position.x, riggedPose.Hips.position.y + 1, -riggedPose.Hips.position.z, 1, 0.07);

    rotate("Chest", riggedPose.Spine, 0.25, 0.3);
    rotate("Spine", riggedPose.Spine, 0.45, 0.3);

    rotate("RightUpperArm", riggedPose.RightUpperArm, 1, 0.3);
    rotate("RightLowerArm", riggedPose.RightLowerArm, 1, 0.3);
    rotate("LeftUpperArm", riggedPose.LeftUpperArm, 1, 0.3);
    rotate("LeftLowerArm", riggedPose.LeftLowerArm, 1, 0.3);

    rotate("LeftUpperLeg", riggedPose.LeftUpperLeg, 1, 0.3);
    rotate("LeftLowerLeg", riggedPose.LeftLowerLeg, 1, 0.3);
    rotate("RightUpperLeg", riggedPose.RightUpperLeg, 1, 0.3);
    rotate("RightLowerLeg", riggedPose.RightLowerLeg, 1, 0.3);
}

\end{lstlisting}
The pseudocode ,\ref{lst:restbody} you have posted describes an animation process for a VRM body. 
The code checks if the \emph{pose2DLandmarks} and \emph{pose3DLandmarks} are available. If they are both available, 
the code calculates the rigged pose using the function \texttt{solvePose} with the \emph{pose3DLandmarks}, \emph{pose2DLandmarks}, 
\emph{mediapipe}, and \emph{videoElement} as input parameters. The function \texttt{solvePose} returns the rigged pose, which 
is then stored in the variable \texttt{riggedPose}.
The code then rotates and positions various body parts of the VRM model based on the values obtained from the 
rigged pose. For example, the hips are rotated based on the value of \texttt{riggedPose.Hips.rotation} with a weight 
of 0.7, and their position is changed to \texttt{x: -riggedPose.Hips.position.x, y: riggedPose.Hips.position.y + 1, z: 
-riggedPose.Hips.position.z} with a weight of 1 and a smoothing factor of 0.07. Similar operations are performed 
for the chest, spine, arms, and legs.
The rotation and position of each body part is performed using two separate functions: \texttt{rotate} and 
\texttt{position}. The first parameter of each function specifies the name of the body part to be animated, 
while the remaining parameters specify the values used for rotation or position, weight, and smoothing factor.
In this way, the VRM model's body is animated according to the solved pose information.
\\
\\
\subsection{Animation of both hands}
\setauthor{Romeo Bhuiyan}
\begin{lstlisting}[language=Python,caption=Animating the left hand,label=lst:hands]
    if (leftHandLandmarks) {
        # Obtain the rotation information of the left hand
        riggedLeftHand = Hand.solve(leftHandLandmarks, "Left");
      
        # Apply the rotation to the left hand rig
        rigRotation("LeftHand", {
          z: riggedLeftHand.LeftWrist.z,
          y: riggedLeftHand.LeftWrist.y,
          x: riggedLeftHand.LeftWrist.x,
        });
      
        # Apply the rotation to each of the finger bones
        rigRotation("LeftRingProximal", riggedLeftHand.LeftRingProximal);
        rigRotation("LeftRingIntermediate", riggedLeftHand.LeftRingIntermediate);
        rigRotation("LeftRingDistal", riggedLeftHand.LeftRingDistal);
        rigRotation("LeftIndexProximal", riggedLeftHand.LeftIndexProximal);
        rigRotation("LeftIndexIntermediate", riggedLeftHand.LeftIndexIntermediate);
        rigRotation("LeftIndexDistal", riggedLeftHand.LeftIndexDistal);
      }
\end{lstlisting}
The provided pseudocode \ref{lst:hands} animates the left (same for the right) hand of a VRM character. 
It starts by checking if the left hand landmarks have been detected. 
If they have, the code then calculates the rotations of the left hand by 
using the \texttt{Hand.solve} function, which takes in the left hand landmarks and the string \texttt{Left}.
The resulting rotations are then applied to the left hand rig and each of the finger 
bones using the \texttt{rigRotation} function. For the left hand rig, the function is called with 
the string \texttt{LeftHand} and the rotation values for the x, y, and z axes, obtained from the 
\texttt{riggedLeftHand} object.
Similarly, the rotations of each finger bone are applied by calling the \texttt{rigRotation} function 
with the names of the bones, such as \texttt{LeftRingProximal},\texttt{LeftRingIntermediate}, and \texttt{LeftRingDistal}, 
and the corresponding rotation values obtained from the \texttt{riggedLeftHand} object. The same process is 
repeated for each of the other finger bones.